{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# US Immigration data analysis\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "In this project, we will analyze the immigration data in the different cities of the United States of America over the years. More precisely, we will specifically analyze the immigration volumes by air transportation for analytical purposes for an airline company or the government. For example, if the government knows that a given city has growing visitors coming to this city, it can improve the infrastructure of the airports in that city, etc...\n",
    "\n",
    "The project will be structured by the following steps:\n",
    "* Step 1: Scope of the Project and Data description\n",
    "* Step 2: Exploring and Assessing the Data\n",
    "* Step 3: Defining the Data Model\n",
    "* Step 4: Runing ETL to Model the Data\n",
    "* Step 5: Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope of the Project and Data description\n",
    "\n",
    "#### Scope of the project\n",
    "\n",
    "In this project, I plan to study the immigration volumes in the US cities by month for analytical purposes, to answer queries like : what US city has been the most the visited for a given year and month ? How many people did come to that city ? What are the features of this city (number of population, average age of the population, etc...).\n",
    "\n",
    "#### Description of the data :\n",
    "\n",
    "To that end, we will use 3 data sources :\n",
    "- the **I94 immigration data** : this data comes from the US National Tourism and Trade Office (see [here](https://travel.trade.gov/research/reports/i94/historical/2016.html)). This dataset contains information about the entries into the US territory : the date of entry, the city of arrival, the gender of the immigrant, his age, etc... This data is not an open data, and actually quite expensive, that's why we will only use the data of the year 2016 (which is provided by Udacity) ;\n",
    "- the **U.S. City Demographic Data** : as its name indicates, this data is about the population data in the US cities, and comes from the US Census Bureau's 2015 American Community Survey ( for more info, see [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)) ;\n",
    "- the **World Temperature Data** : this dataset contains information about the temperatures in the world cities since 1743, and comes from Kaggle (for more info, see [here](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data)) ;\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import s3fs\n",
    "import json\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "from pyspark.sql.functions import avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#! pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# aws credentials\n",
    "AWS_ACCESS_KEY = ''\n",
    "AWS_SECRET_KEY = ''\n",
    "\n",
    "# s3 output path for saving files\n",
    "S3_PATH = 's3://udacity/capstone/project/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# s3 filesystem to save file in s3\n",
    "fs = s3fs.S3FileSystem(key=AWS_ACCESS_KEY, secret=AWS_SECRET_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Exploring and Assessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Exploring the Temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8599212, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "temperatures_df = pd.read_csv(fname)\n",
    "temperatures_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temperatures_df['dt'] = pd.to_datetime(temperatures_df['dt'], format='%Y-%m-%d')\n",
    "temperatures_df['year'] = temperatures_df['dt'].dt.year\n",
    "temperatures_df['month'] = temperatures_df['dt'].dt.month\n",
    "\n",
    "temperatures_df.sort_values(['year', 'month'], ascending=[True, True], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "      <td>1743</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3239</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>10.013</td>\n",
       "      <td>2.291</td>\n",
       "      <td>Çorlu</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>40.99N</td>\n",
       "      <td>27.69E</td>\n",
       "      <td>1743</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0    1743-11-01               6.068                          1.737  Århus   \n",
       "3239 1743-11-01              10.013                          2.291  Çorlu   \n",
       "\n",
       "      Country Latitude Longitude  year  month  \n",
       "0     Denmark   57.05N    10.33E  1743     11  \n",
       "3239   Turkey   40.99N    27.69E  1743     11  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperatures_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8595972</th>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zuwarah</td>\n",
       "      <td>Libya</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>12.45E</td>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599211</th>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                dt  AverageTemperature  AverageTemperatureUncertainty  \\\n",
       "8595972 2013-09-01                 NaN                            NaN   \n",
       "8599211 2013-09-01                 NaN                            NaN   \n",
       "\n",
       "            City      Country Latitude Longitude  year  month  \n",
       "8595972  Zuwarah        Libya   32.95N    12.45E  2013      9  \n",
       "8599211   Zwolle  Netherlands   52.24N     5.26E  2013      9  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperatures_df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Dataset description : \n",
    "This dataset contains information of world temperature data from November 1743 to September 2013. However our US immigration data contains data of 2016, so we'll approximate the temperatures in the US cities by the last information available for them in this dataset for every month. Moreover, we can see that there are some null values in the **AverageTemperature** column, so we will first remove these null values before filtering the last temperature information for US cities. These cleaning steps will be done in the ETL pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Exploring the US cities demographics data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2891, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_cities_demographics = pd.read_csv('us-cities-demographics.csv', sep=';')\n",
    "us_cities_demographics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_cities_demographics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Dataset description : \n",
    "By watching this dataset, we can see that each row of the dataset corresponds to the population data for a city with a zoom on the population of a particular race (White, Black or African-American, Hispanic or Latino, Asian, etc..). But we are actually not interested in the distinction between races, so we will deduplicate the lines by the couple (City, State) because we can have the same city name in different states. These steps will be down in the ETL pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_us_cities_demographics_data(us_cities_demographics_df):\n",
    "    ### deduplicate the lines\n",
    "    us_cities_demographics_deduplicated = us_cities_demographics_df.drop_duplicates(subset=['City', 'State'])\n",
    "    \n",
    "    ### keep only the useful columns\n",
    "    us_cities_demographics_deduplicated = us_cities_demographics_deduplicated[['City', 'State', 'Median Age', 'Male Population', 'Female Population', \n",
    "                                                                               'Total Population', 'Foreign-born', 'Average Household Size', 'State Code']]\n",
    "    \n",
    "    return us_cities_demographics_deduplicated\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "us_cities_demographics_deduplicated = clean_us_cities_demographics_data(us_cities_demographics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Exploring the i94 immigration data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample_data = pd.read_csv('immigration_data_sample.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>867733</th>\n",
       "      <th>237247</th>\n",
       "      <th>3049432</th>\n",
       "      <th>1908985</th>\n",
       "      <th>2714770</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cicid</th>\n",
       "      <td>1.80340e+06</td>\n",
       "      <td>480428</td>\n",
       "      <td>5.70311e+06</td>\n",
       "      <td>3.861e+06</td>\n",
       "      <td>5.4711e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94yr</th>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94mon</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94cit</th>\n",
       "      <td>135</td>\n",
       "      <td>148</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94res</th>\n",
       "      <td>135</td>\n",
       "      <td>112</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94port</th>\n",
       "      <td>ATL</td>\n",
       "      <td>MIA</td>\n",
       "      <td>PBB</td>\n",
       "      <td>LOS</td>\n",
       "      <td>SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrdate</th>\n",
       "      <td>20554</td>\n",
       "      <td>20547</td>\n",
       "      <td>20574</td>\n",
       "      <td>20565</td>\n",
       "      <td>20573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94mode</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94addr</th>\n",
       "      <td>GA</td>\n",
       "      <td>FL</td>\n",
       "      <td>FL</td>\n",
       "      <td>NY</td>\n",
       "      <td>WA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depdate</th>\n",
       "      <td>20566</td>\n",
       "      <td>20559</td>\n",
       "      <td>20589</td>\n",
       "      <td>20571</td>\n",
       "      <td>20580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94bir</th>\n",
       "      <td>77</td>\n",
       "      <td>51</td>\n",
       "      <td>45</td>\n",
       "      <td>18</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94visa</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dtadfile</th>\n",
       "      <td>20160410</td>\n",
       "      <td>20160403</td>\n",
       "      <td>20160430</td>\n",
       "      <td>20160421</td>\n",
       "      <td>20160429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visapost</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>occup</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entdepa</th>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>Z</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entdepd</th>\n",
       "      <td>O</td>\n",
       "      <td>I</td>\n",
       "      <td>K</td>\n",
       "      <td>O</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entdepu</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matflag</th>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biryear</th>\n",
       "      <td>1939</td>\n",
       "      <td>1965</td>\n",
       "      <td>1971</td>\n",
       "      <td>1998</td>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dtaddto</th>\n",
       "      <td>07082016</td>\n",
       "      <td>07012016</td>\n",
       "      <td>07282016</td>\n",
       "      <td>07192016</td>\n",
       "      <td>07272016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>F</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insnum</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airline</th>\n",
       "      <td>DL</td>\n",
       "      <td>LH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VS</td>\n",
       "      <td>BR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admnum</th>\n",
       "      <td>5.59237e+10</td>\n",
       "      <td>5.55509e+10</td>\n",
       "      <td>1.75454e+10</td>\n",
       "      <td>5.65094e+10</td>\n",
       "      <td>5.95016e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fltno</th>\n",
       "      <td>00688</td>\n",
       "      <td>00462</td>\n",
       "      <td>LAND</td>\n",
       "      <td>00003</td>\n",
       "      <td>00026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visatype</th>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              867733       237247       3049432      1908985      2714770\n",
       "cicid     1.80340e+06       480428  5.70311e+06    3.861e+06   5.4711e+06\n",
       "i94yr            2016         2016         2016         2016         2016\n",
       "i94mon              4            4            4            4            4\n",
       "i94cit            135          148          135          135          207\n",
       "i94res            135          112          135          135          207\n",
       "i94port           ATL          MIA          PBB          LOS          SEA\n",
       "arrdate         20554        20547        20574        20565        20573\n",
       "i94mode             1            1            3            1            1\n",
       "i94addr            GA           FL           FL           NY           WA\n",
       "depdate         20566        20559        20589        20571        20580\n",
       "i94bir             77           51           45           18           77\n",
       "i94visa             2            2            2            2            2\n",
       "count               1            1            1            1            1\n",
       "dtadfile     20160410     20160403     20160430     20160421     20160429\n",
       "visapost          NaN          NaN          NaN          NaN          NaN\n",
       "occup             NaN          NaN          NaN          NaN          NaN\n",
       "entdepa             G            O            Z            G            G\n",
       "entdepd             O            I            K            O            I\n",
       "entdepu           NaN          NaN          NaN          NaN          NaN\n",
       "matflag             M            M            M            M            M\n",
       "biryear          1939         1965         1971         1998         1939\n",
       "dtaddto      07082016     07012016     07282016     07192016     07272016\n",
       "gender              M          NaN            M            F            M\n",
       "insnum            NaN          NaN          NaN          NaN          NaN\n",
       "airline            DL           LH          NaN           VS           BR\n",
       "admnum    5.59237e+10  5.55509e+10  1.75454e+10  5.65094e+10  5.95016e+10\n",
       "fltno           00688        00462         LAND        00003        00026\n",
       "visatype           WT           WT           WT           WT           WT"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data.sample(5).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### I94 immigration dataset description : \n",
    "This dataset actually contains a lot of information with each record corresponding to one person's travel to the united states. We have a lot of data that is not useful for us like for example the arrival date, the insurance number, etc... so we'll only use a handful of features for our analysis. Also, we don't have to drop duplicate lines here, but we need to aggregate the data at the level of month and city. The city is already available, and we can retrieve the city and the state thanks to the data dicitonnary available (*I94_SAS_Labels_Descriptions.SAS*). These steps will be down in the ETL pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Considering the type of analysis we want to do, the best data model will be a data model in a star schema, with one main fact table (the immigration_table) and two dimension tables (the city_table and the temperature_table). This architecture is a good choice here because the main analysis we want to do is computing how much people travelled to a given city in a given period of time (month(s)/year), and it is possible to do it with the fact table only, but if we want more information about the city like the number of population or the average temperature, we can do joins with these dimension tables.\n",
    "\n",
    "##### Dimension table : temperature_table\n",
    "* temperature_id : primary key\n",
    "* City\n",
    "* month\n",
    "* AverageTemperature\n",
    "\n",
    "##### Dimension table : city_table\n",
    "* city_id : id which identifies uniquely a city (with the correspoding state)\n",
    "* City\n",
    "* State\n",
    "* State Code\n",
    "* Median Age\n",
    "* Male Population\n",
    "* Female Population\n",
    "* Total Population\n",
    "* Foreign-born\n",
    "* Average Household Size\n",
    "\n",
    "##### Fact table : immigration_table\n",
    "* immigration_id : primary_key\n",
    "* city_id : id which identifies uniquely a city (with the correspoding state)\n",
    "* City : city, useful to join the temperature table \n",
    "* Month\n",
    "* year\n",
    "* Nb_passenger : number of passengers\n",
    "* Average_age : average age of the passengers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Chosen data model\n",
    "\n",
    "The chosen data model here is that of a **datalake on s3**, and the reasons for this choice are the following :\n",
    "* No need to access the data quickly, as it will be used for static analysis only\n",
    "* The data are not big (even if the immigration data may seem to be very big, after the aggregations by month and city, it will have below 5000 lines), so charging the files from s3 eill be very fast\n",
    "* It is less expensive than other solutions like databases such as Amazon Redshift or Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.3 Mapping Out Data Pipelines\n",
    "\n",
    "The steps necessary to pipeline the data into the chosen data model are the following:\n",
    "* Read all the immigration data with Spark, as the data is big (approximately 3 million lines for 1 month of data), add the columns of city and state for the merge with the US Cities Demographic dataset, and aggregate the data according to the needs;\n",
    "* Clean and process the Us demographics data to create the city table;\n",
    "* Clean and process the world temperature data to create the temperature table;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "\n",
    "For information, the entire ETL pipeline has been refactored in the *etl.py* script, so we can directly run it in a terminal to generate and save the fact and dimensions table. But for explanation purposes and better clarity, we will run the ETL pipeline below also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning and processing the temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_and_process_temperature_data(temperatures_data_path, save_local=True, fs=None, save_to_s3=False, output_path=None):\n",
    "    '''\n",
    "    Cleans and processes the temperature data.\n",
    "    If the s3 arguments are provided, also saves the final temperature table to s3.\n",
    "    '''\n",
    "\n",
    "    temperatures_df = pd.read_csv(temperatures_data_path)\n",
    "\n",
    "    # format the date and extract the year and month\n",
    "    temperatures_df['dt'] = pd.to_datetime(temperatures_df['dt'], format='%Y-%m-%d')\n",
    "    temperatures_df['year'] = temperatures_df['dt'].dt.year\n",
    "    temperatures_df['month'] = temperatures_df['dt'].dt.month\n",
    "\n",
    "    temperatures_df.sort_values(['year', 'month'], ascending=[True, True], inplace=True)\n",
    "\n",
    "    # Remove null values in the AverageTemperature column\n",
    "    temperatures_df = temperatures_df[pd.notnull(temperatures_df['AverageTemperature'])]\n",
    "\n",
    "    # Filter on the US\n",
    "    temperatures_US = temperatures_df[temperatures_df['Country'] == 'United States']\n",
    "\n",
    "    # Keep the last information\n",
    "    temperatures_US.drop_duplicates(subset=['City', 'month'], keep='last', inplace=True)\n",
    "\n",
    "    # Create the temperature id\n",
    "    temperatures_US.index = np.arange(1, len(temperatures_US) + 1)\n",
    "    temperatures_US = temperatures_US.reset_index()\n",
    "    temperatures_US = temperatures_US.rename(columns={'index': 'temperature_id'})\n",
    "\n",
    "    # Keeping the useful information\n",
    "    temperatures_US = temperatures_US[['temperature_id', 'City', 'month', 'AverageTemperature']]\n",
    "\n",
    "    if save_local:\n",
    "        temperatures_US.to_csv('temperature_table.csv', sep=';', index=False)\n",
    "\n",
    "    if save_to_s3:\n",
    "        with fs.open(output_path + 'temperature_table.csv', 'wb') as output_file:\n",
    "            temperatures_US.to_csv(output_file, sep=';', index=False)\n",
    "\n",
    "    return temperatures_US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temperature_data_path = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "\n",
    "temperature_table = clean_and_process_temperature_data(temperature_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning and processing the us demographics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_and_process_us_cities_demographics_data(us_cities_demographics_path, save_local=True, fs=None, save_to_s3=False, output_path=None):\n",
    "    '''\n",
    "\n",
    "    Cleans and processes the us demographics data.\n",
    "    If the s3 arguments are provided, also saves the final demographics table to s3 and/or locally.\n",
    "    '''\n",
    "\n",
    "    us_cities_demographics_df = pd.read_csv(us_cities_demographics_path, sep=';')\n",
    "\n",
    "    # deduplicate the lines\n",
    "    us_cities_demographics_deduplicated = us_cities_demographics_df.drop_duplicates(subset=['City', 'State'])\n",
    "\n",
    "    # create city_id for the join with the immigration table\n",
    "    us_cities_demographics_deduplicated['City'] = us_cities_demographics_deduplicated['City'].str.lower()\n",
    "    us_cities_demographics_deduplicated['city_id'] = us_cities_demographics_deduplicated[['City', 'State Code']].apply(\n",
    "        lambda x: hash(tuple(x)), axis=1)\n",
    "\n",
    "    # keep only the useful columns\n",
    "    us_cities_demographics_deduplicated_final = us_cities_demographics_deduplicated[\n",
    "        ['city_id', 'City', 'State', 'State Code', 'Median Age', 'Male Population',\n",
    "         'Female Population', 'Total Population', 'Foreign-born', 'Average Household Size']]\n",
    "\n",
    "    if save_local:\n",
    "        us_cities_demographics_deduplicated_final.to_csv('demographics_table.csv', sep=';', index=False)\n",
    "\n",
    "    if save_to_s3:\n",
    "        with fs.open(output_path + 'demographics_table.csv', 'wb') as output_file:\n",
    "            us_cities_demographics_deduplicated_final.to_csv(output_file, sep=';', index=False)\n",
    "\n",
    "    return us_cities_demographics_deduplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "us_cities_demographics_data_path = 'us-cities-demographics.csv'\n",
    "\n",
    "cities_table = clean_and_process_us_cities_demographics_data(us_cities_demographics_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning and processing the us immigrations data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_and_process_immigration_data(immigration_data_path, save_local=True, fs=None, save_to_s3=False, output_path=None):\n",
    "    '''\n",
    "\n",
    "    Cleans and processes the us immigration data.\n",
    "    If the s3 arguments are provided, also saves the final immigration table to s3 and/or locally.\n",
    "    '''\n",
    "\n",
    "    spark = SparkSession.builder. \\\n",
    "        config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "        .enableHiveSupport().getOrCreate()\n",
    "\n",
    "    final_df_spark = spark.read.format('com.github.saurfang.sas.spark').load(\n",
    "        immigration_data_path+'i94_jan16_sub.sas7bdat')\n",
    "    final_df_spark_columns = list(final_df_spark.columns)\n",
    "\n",
    "    months = ['feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "\n",
    "    for month in months:\n",
    "        file_path = immigration_data_path + 'i94_' + month + '16_sub.sas7bdat'\n",
    "        df_spark = spark.read.format('com.github.saurfang.sas.spark').load(file_path)\n",
    "        df_spark = df_spark.select(final_df_spark_columns)\n",
    "        final_df_spark = final_df_spark.union(df_spark)\n",
    "\n",
    "    # filter only air transportation\n",
    "    final_df_spark = final_df_spark[final_df_spark['i94mode'] == 1]\n",
    "\n",
    "    # aggregate data\n",
    "    immigration_data_aggregated = final_df_spark.groupBy(\"i94yr\", \"i94mon\", \"i94port\").agg(_sum(\"count\"), avg('i94bir'))\n",
    "\n",
    "    # convert to pandas dataframe\n",
    "    pd_immigration_data_aggregated_df = immigration_data_aggregated.toPandas()\n",
    "\n",
    "    # Add the information about city and State to create city_id\n",
    "    with open('mapping_port_city_state.json', 'r') as fp:\n",
    "        map_port_city = json.load(fp)\n",
    "\n",
    "    mapping_port_city_df = pd.DataFrame(\n",
    "        {'i94port': list(map_port_city.keys()), 'City, State Code': list(map_port_city.values())})\n",
    "\n",
    "    # Create City ans State Code from the data dictionary of the I94 data\n",
    "    mapping_port_city_df['City'] = mapping_port_city_df['City, State Code'].map(lambda x: str(x).split(',')[0])\n",
    "\n",
    "    mapping_port_city_df['State Code'] = mapping_port_city_df['City, State Code'].map(\n",
    "        lambda x: str(x).split(',')[1] if len(str(x).split(',')) > 1 else pd.np.NaN)\n",
    "    mapping_port_city_df['State Code'] = mapping_port_city_df['State Code'].str.strip()\n",
    "    mapping_port_city_df['State Code'] = mapping_port_city_df['State Code'].map(lambda x: str(x).split(' ')[0])\n",
    "\n",
    "    pd_immigration_data_aggregated_df_final = pd.merge(pd_immigration_data_aggregated_df,\n",
    "                                                       mapping_port_city_df[['i94port', 'City', 'State Code']],\n",
    "                                                       on='i94port',\n",
    "                                                       how='left')\n",
    "\n",
    "    pd_immigration_data_aggregated_df_final['City'] = pd_immigration_data_aggregated_df_final['City'].str.lower()\n",
    "\n",
    "    pd_immigration_data_aggregated_df_final['city_id'] = pd_immigration_data_aggregated_df_final[\n",
    "        ['City', 'State Code']].apply(lambda x: hash(tuple(x)), axis=1)\n",
    "\n",
    "    # Create primary key\n",
    "    pd_immigration_data_aggregated_df_final.index = np.arange(1, len(pd_immigration_data_aggregated_df_final) + 1)\n",
    "    pd_immigration_data_aggregated_df_final = pd_immigration_data_aggregated_df_final.reset_index()\n",
    "\n",
    "    # rename the columns\n",
    "    pd_immigration_data_aggregated_df_final = pd_immigration_data_aggregated_df_final.rename(columns={'i94yr': 'Year',\n",
    "                                                                                                      'i94mon': 'Month',\n",
    "                                                                                                      'sum(count)': 'Nb_passengers',\n",
    "                                                                                                      'avg(i94bir)': 'Average_age',\n",
    "                                                                                                      'index': 'immigration_id'})\n",
    "\n",
    "    pd_immigration_data_aggregated_df_final = pd_immigration_data_aggregated_df_final[\n",
    "        ['immigration_id', 'city_id', 'City',\n",
    "         'Year', 'Month',\n",
    "         'Nb_passengers', 'Average_age']]\n",
    "\n",
    "    if save_local:\n",
    "        pd_immigration_data_aggregated_df_final.to_csv('immigration_table.csv', sep=';', index=False)\n",
    "\n",
    "    if save_to_s3:\n",
    "        with fs.open(output_path + 'immigration_table.csv', 'wb') as output_file:\n",
    "            pd_immigration_data_aggregated_df_final.to_csv(output_file, sep=';', index=False)\n",
    "\n",
    "    return pd_immigration_data_aggregated_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_data_path = '../../data/18-83510-I94-Data-2016/'\n",
    "\n",
    "immigration_table = clean_and_process_immigration_data(immigration_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "We will do 2 data quality checks to ensure that we don't have errors on the data, and the pipeline ran without errors.\n",
    "\n",
    "\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "* => Checking that the temperatures are always positive\n",
    "* => Checking that we have information on all the cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1 - Check that each key in the city table (city_id) is unique\n",
    "\n",
    "By construction, it is the case for the temperature_id and immigration_id (as they are created from the indexes), but as the city_id is created by hashing the city and the state, we have to check that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "city_table = pd.read_csv('demographics_table.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good News, each city_id is unique !\n"
     ]
    }
   ],
   "source": [
    "if city_table.city_id.nunique()== len(city_table):\n",
    "    print('Good News, each city_id is unique !')\n",
    "else:\n",
    "    print('There is an error in the city table, the primary keys are not unique !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Fortunately each key is unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2 - Check that we have information about all the cities of the immigration table in the city table\n",
    "\n",
    "This is not something which is obvious, because the 2 tables come from different sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_table =pd.read_csv('immigration_table.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "city_table = pd.read_csv('demographics_table.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "final_table = pd.merge(immigration_table, city_table[['city_id', 'State']],on = 'city_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 62.0% missing data on cities in the city table !\n"
     ]
    }
   ],
   "source": [
    "if final_table.State.isnull().sum()== 0:\n",
    "    print('Good News, we have information for all the cities in the immigration table !')\n",
    "else:\n",
    "    missing_percentage = final_table.State.isnull().sum()/len(final_table)\n",
    "    print(f'There is {100*round(missing_percentage, 2)}% missing data on cities in the city table !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Well, it seems we are missing information on 62% of the citis in the immigration table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "You can find here a brief description of what the data is and where it came from, and also a small explanation for each field.\n",
    "\n",
    "##### Dimension table : temperature_table (comes from the World Temperature Data from Kaggle)\n",
    "* temperature_id : primary key\n",
    "* City \n",
    "* month\n",
    "* AverageTemperature\n",
    "\n",
    "##### Dimension table : city_table\n",
    "* city_id : id which identifies uniquely a city (with the corresponding state), created by creating a hash of the city + State Code\n",
    "* City\n",
    "* State\n",
    "* State Code\n",
    "* Median Age\n",
    "* Male Population\n",
    "* Female Population\n",
    "* Total Population\n",
    "* Foreign-born\n",
    "* Average Household Size\n",
    "\n",
    "##### Fact table : immigration_table\n",
    "* immigration_id : primary_key\n",
    "* city_id : id which identifies uniquely a city (with the correspoding state)\n",
    "* City : city, useful to join with the temperature table \n",
    "* Month\n",
    "* year\n",
    "* Nb_passenger : number of passengers\n",
    "* Average_age : average age of the passengers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 5: Project Write Up\n",
    "### Rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "* For this project, we used Spark to analyze and aggregate the immigration data, because one month of is approximately 3 millions rows, so 12 months would have been 36 million rows. To work with this large amount of data, Spark is a good choice as it will work more efficiently than tools like pandas, which may have been ok for this analysis too, but may it should have needed more RAM and take more time to do the same job. I have also chosen a data model **datalake on s3**, for the following reasons  : no need to access the data quickly, as it will be used for static analysis only; the data are not big , so charging the files from s3 will be very fast and it is less expensive than other solutions such as Amazon Redshift or Postgres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* How often the data should be updated : for this project, the data should be updated once a month, because we are doing analysis of immigration data by city and by month, so it will be logical to update it monthly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* How I would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x : in this case, we will be dealing with big data, so the solution would a NoSQL database, and consequently we will have to model the database according to the queries we want to run. \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day : in this case, I will use a tool like Airflow which is great for automation and monitoring.\n",
    " * The database needed to be accessed by 100+ people : in this case, we may have consistency issues, so we'll have to use a relational databases like Postgres or Amazon Redshift to solve this issue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
